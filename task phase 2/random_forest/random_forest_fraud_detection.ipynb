{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Fraud Detection\n",
    "\n",
    "Random Forest classifier using scikit-learn for online payment fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                           roc_auc_score, confusion_matrix, classification_report, roc_curve)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../onlinefraud.csv')\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'\\nDataset Info:')\n",
    "df.info()\n",
    "\n",
    "print(f'\\nFirst 5 rows:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Target variable distribution:')\n",
    "fraud_counts = df['isFraud'].value_counts()\n",
    "print(fraud_counts)\n",
    "print(f'\\nFraud percentage: {fraud_counts[1] / len(df) * 100:.4f}%')\n",
    "\n",
    "print(f'\\nMissing values:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "\n",
    "# Drop identifiers and flagged fraud (contains leakage)\n",
    "data = data.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1)\n",
    "\n",
    "# One-hot encode type\n",
    "data = pd.get_dummies(data, columns=['type'], prefix='type')\n",
    "\n",
    "# NO engineered features - they cause data leakage\n",
    "print(f'Total features: {data.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['isFraud'], axis=1)\n",
    "y = data['isFraud'].values\n",
    "\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')\n",
    "\n",
    "# Sample 1M for better statistics\n",
    "sample_size = 1000000\n",
    "X_sample, _, y_sample, _ = train_test_split(\n",
    "    X, y, test_size=1 - sample_size/len(X), stratify=y, random_state=42\n",
    ")\n",
    "print(f'\\nSampled: {X_sample.shape[0]} rows')\n",
    "print(f'Fraud ratio: {y_sample.sum() / len(y_sample) * 100:.4f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val/Test split: 60/20/20\n",
    "# First split: 60% train+val, 40% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.2, stratify=y_sample, random_state=42\n",
    ")\n",
    "# Second split: 75% train, 25% val (of the 80%, this gives us 60/20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'Train: {X_train.shape[0]} samples')\n",
    "print(f'Val: {X_val.shape[0]} samples')\n",
    "print(f'Test: {X_test.shape[0]} samples')\n",
    "print(f'Train fraud ratio: {y_train.sum() / len(y_train) * 100:.4f}%')\n",
    "print(f'Val fraud ratio: {y_val.sum() / len(y_val) * 100:.4f}%')\n",
    "print(f'Test fraud ratio: {y_test.sum() / len(y_test) * 100:.4f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print('Training Random Forest...')\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f'\\nModel trained with {rf.n_estimators} trees')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test_scaled)\n",
    "y_proba = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print('PERFORMANCE METRICS')\n",
    "print('=' * 50)\n",
    "print(f'Accuracy:  {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall:    {recall:.4f}')\n",
    "print(f'F1-Score:  {f1:.4f}')\n",
    "print(f'ROC-AUC:   {roc_auc:.4f}')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=['Non-Fraud', 'Fraud']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Tune threshold on VALIDATION set (NOT test set!)\n",
    "y_val_proba = rf.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f'Optimal threshold (from VALIDATION): {optimal_threshold:.4f}')\n",
    "\n",
    "# Evaluate on TEST set with tuned threshold\n",
    "y_test_proba = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred = (y_test_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Calculate final metrics on TEST set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print(f'\\nFINAL TEST SET METRICS (with tuned threshold):')\n",
    "print('=' * 50)\n",
    "print(f'Accuracy:  {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall:    {recall:.4f}')\n",
    "print(f'F1-Score:  {f1:.4f}')\n",
    "print(f'ROC-AUC:   {roc_auc:.4f}')\n",
    "print('=' * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Random Forest')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "values = [accuracy, precision, recall, f1, roc_auc]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(metrics, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'], alpha=0.7)\n",
    "plt.ylim([0.85, 1.0])\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics - Random Forest')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print('Top 15 Most Important Features:')\n",
    "print(importance_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = importance_df.head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(range(len(top_features)), top_features['importance'], \n",
    "                color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Most Important Features - Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "for i, (bar, importance) in enumerate(zip(bars, top_features['importance'])):\n",
    "    plt.text(importance + 0.001, i, f'{importance:.3f}', \n",
    "             va='center', fontsize=9)\n",
    "\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RANDOM FOREST FRAUD DETECTION - SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'\\nModel Performance:')\n",
    "print(f'  Accuracy: {accuracy:.4f}')\n",
    "print(f'  Precision: {precision:.4f}')\n",
    "print(f'  Recall: {recall:.4f}')\n",
    "print(f'  F1-Score: {f1:.4f}')\n",
    "print(f'  ROC-AUC: {roc_auc:.4f}')\n",
    "print(f'\\nKey Insights:')\n",
    "print(f'  - Top features: {importance_df.head(3)[\"feature\"].tolist()}')\n",
    "print(f'  - Model trained on {len(X_train)} samples')\n",
    "print(f'  - Test set: {len(X_test)} samples with {y_test.sum()} fraud cases')\n",
    "print(f'  - Using class_weight=\\'balanced\\' to handle imbalance')\n",
    "print('=' * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}