{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1b7e8b50",
      "metadata": {},
      "source": [
        "# Character-Level LSTM for The Office (US) Dialogue\n",
        "\n",
        "This notebook trains a character-level LSTM language model to generate script-style dialogue in the format:\n",
        "\n",
        "```\n",
        "PERSON: Dialogue\n",
        "```\n",
        "\n",
        "It is fully runnable in Google Colab and will use GPU if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d591b44f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if DEVICE.type == \"cuda\":\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f70af9c2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-50d8229c-3db7-43a8-b7d1-31a3e6c76b19\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-50d8229c-3db7-43a8-b7d1-31a3e6c76b19\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "DATA_PATH = Path(\"office_script_clean.txt\")\n",
        "CLEANED_PATH = Path(\"office_script_cleaned.txt\")\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    print(\"office_script_clean.txt not found in current directory.\")\n",
        "    print(\"\\nTo upload in Colab:\")\n",
        "    print(\"  1. Click the folder icon in the left sidebar\")\n",
        "    print(\"  2. Click the upload icon\")\n",
        "    print(\"  3. Select office_script_clean.txt\")\n",
        "    print(\"  4. Wait for upload to complete, then rerun this cell\")\n",
        "    print(\"\\nOr drag and drop the file into the Colab file browser\")\n",
        "    raise FileNotFoundError(\n",
        "        f\"office_script_clean.txt not found at {DATA_PATH.absolute()}. \"\n",
        "        \"Please upload it using Colab's Files sidebar (see instructions above) and rerun this cell.\"\n",
        "    )\n",
        "\n",
        "print(f\"Found {DATA_PATH}\")\n",
        "raw_text = DATA_PATH.read_text(encoding=\"utf-8\")\n",
        "print(f\"Loaded {len(raw_text):,} characters from dataset\")\n",
        "\n",
        "def clean_line(line: str) -> str:\n",
        "    line = line.replace(\"\\r\", \"\")\n",
        "    line = re.sub(r\"<[^>]+>\", \"\", line)\n",
        "    line = re.sub(r\"^\\s*(\\d{1,2}:\\d{2}|\\d{1,2}:\\d{2}:\\d{2})\\s*\", \"\", line)\n",
        "    line = re.sub(r\"\\[[^\\]]*\\]\", \"\", line)\n",
        "    line = re.sub(r\"\\([^\\)]*\\)\", \"\", line)\n",
        "    if \":\" not in line:\n",
        "        return \"\"\n",
        "    speaker, dialogue = line.split(\":\", 1)\n",
        "    speaker = speaker.strip().upper()\n",
        "    dialogue = dialogue.strip()\n",
        "    if not speaker or not dialogue:\n",
        "        return \"\"\n",
        "    dialogue = re.sub(r\"\\s+([,.!?;:])\", r\"\\1\", dialogue)\n",
        "    dialogue = re.sub(r\"\\s{2,}\", \" \", dialogue)\n",
        "    return f\"{speaker}: {dialogue}\"\n",
        "\n",
        "lines = []\n",
        "for raw_line in raw_text.split(\"\\n\"):\n",
        "    cleaned = clean_line(raw_line)\n",
        "    if cleaned:\n",
        "        lines.append(cleaned)\n",
        "\n",
        "ADD_SCENE_SEPARATOR = False\n",
        "if ADD_SCENE_SEPARATOR:\n",
        "    corpus_text = \"\\n\\n===\\n\\n\".join(lines)\n",
        "else:\n",
        "    corpus_text = \"\\n\".join(lines)\n",
        "\n",
        "CLEANED_PATH.write_text(corpus_text, encoding=\"utf-8\")\n",
        "print(f\"Lines kept: {len(lines)}\")\n",
        "print(f\"Cleaned corpus saved to: {CLEANED_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7837e0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "text = CLEANED_PATH.read_text(encoding=\"utf-8\")\n",
        "\n",
        "chars = sorted(set(text))\n",
        "char_to_id = {ch: i for i, ch in enumerate(chars)}\n",
        "id_to_char = {i: ch for ch, i in char_to_id.items()}\n",
        "\n",
        "encoded = torch.tensor([char_to_id[ch] for ch in text], dtype=torch.long)\n",
        "\n",
        "split_idx = int(0.9 * len(encoded))\n",
        "train_data = encoded[:split_idx]\n",
        "val_data = encoded[split_idx:]\n",
        "\n",
        "CONTEXT_LEN = 120\n",
        "\n",
        "print(f\"Vocabulary size: {len(chars)}\")\n",
        "print(f\"Total characters: {len(encoded)}\")\n",
        "print(f\"Context window length: {CONTEXT_LEN}\")\n",
        "print(f\"Train size: {len(train_data)}\")\n",
        "print(f\"Validation size: {len(val_data)}\")\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e9fcaeb",
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomWindowBatchDataset(IterableDataset):\n",
        "    def __init__(self, data: torch.Tensor, context_len: int, batch_size: int, steps_per_epoch: int):\n",
        "        self.data = data\n",
        "        self.context_len = context_len\n",
        "        self.batch_size = batch_size\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __iter__(self):\n",
        "        data = self.data\n",
        "        device = data.device\n",
        "        max_start = len(data) - self.context_len - 1\n",
        "        offsets = torch.arange(self.context_len, device=device)\n",
        "        for _ in range(self.steps_per_epoch):\n",
        "            starts = torch.randint(0, max_start, (self.batch_size,), device=device)\n",
        "            idx = starts[:, None] + offsets[None, :]\n",
        "            x = data[idx]\n",
        "            y = data[idx + 1]\n",
        "            yield x, y\n",
        "\n",
        "\n",
        "class SequentialWindowBatchDataset(IterableDataset):\n",
        "    def __init__(self, data: torch.Tensor, context_len: int, batch_size: int, steps: int):\n",
        "        self.data = data\n",
        "        self.context_len = context_len\n",
        "        self.batch_size = batch_size\n",
        "        self.steps = steps\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.steps\n",
        "\n",
        "    def __iter__(self):\n",
        "        data = self.data\n",
        "        device = data.device\n",
        "        max_start = len(data) - self.context_len - 1\n",
        "        total = self.steps * self.batch_size\n",
        "        offsets = torch.arange(self.context_len, device=device)\n",
        "        for start in range(0, total, self.batch_size):\n",
        "            starts = torch.arange(start, start + self.batch_size, device=device)\n",
        "            idx = starts[:, None] + offsets[None, :]\n",
        "            x = data[idx]\n",
        "            y = data[idx + 1]\n",
        "            yield x, y\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "if DEVICE.type == \"cuda\":\n",
        "    train_data_device = train_data.to(DEVICE)\n",
        "    val_data_device = val_data.to(DEVICE)\n",
        "else:\n",
        "    train_data_device = train_data\n",
        "    val_data_device = val_data\n",
        "\n",
        "max_start_train = len(train_data_device) - CONTEXT_LEN - 1\n",
        "max_start_val = len(val_data_device) - CONTEXT_LEN - 1\n",
        "train_steps = max_start_train // BATCH_SIZE\n",
        "val_steps = max_start_val // BATCH_SIZE\n",
        "\n",
        "train_dataset = RandomWindowBatchDataset(train_data_device, CONTEXT_LEN, BATCH_SIZE, train_steps)\n",
        "val_dataset = SequentialWindowBatchDataset(val_data_device, CONTEXT_LEN, BATCH_SIZE, val_steps)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=None)\n",
        "val_loader = DataLoader(val_dataset, batch_size=None)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e172462",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_layers: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        logits = self.fc(out)\n",
        "        return logits, hidden\n",
        "\n",
        "EMBED_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.25\n",
        "\n",
        "model = CharLSTM(\n",
        "    vocab_size=len(chars),\n",
        "    embed_dim=EMBED_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        ").to(DEVICE)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "165d5931",
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 15\n",
        "LEARNING_RATE = 2e-3\n",
        "CLIP_NORM = 1.0\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "use_amp = DEVICE.type == \"cuda\"\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_ppls = []\n",
        "val_ppls = []\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start = time.time()\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            logits, _ = model(x)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_ppl = math.exp(avg_train_loss)\n",
        "    train_ppls.append(train_ppl)\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE, non_blocking=True)\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                logits, _ = model(x)\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_ppl = math.exp(avg_val_loss)\n",
        "    val_ppls.append(val_ppl)\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"train loss {avg_train_loss:.4f} | train ppl {train_ppl:.4f} | \"\n",
        "        f\"val loss {avg_val_loss:.4f} | val ppl {val_ppl:.4f} | \"\n",
        "        f\"time {epoch_time:.1f}s\",\n",
        "        flush=True,\n",
        "    )\n",
        "\n",
        "print(f\"Final validation perplexity: {val_ppls[-1]:.4f}\")\n",
        "print(\"Expected final validation perplexity range: 2.5-3.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b278109e",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label=\"Train loss\")\n",
        "plt.plot(val_losses, label=\"Val loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cross-entropy loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_ppls, label=\"Train perplexity\")\n",
        "plt.plot(val_ppls, label=\"Val perplexity\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bae2aa0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_script(seed_text, temperature, num_tokens_to_generate):\n",
        "    if temperature <= 0:\n",
        "        raise ValueError(\"temperature must be > 0\")\n",
        "    for ch in seed_text:\n",
        "        if ch not in char_to_id:\n",
        "            raise ValueError(f\"Character not in vocab: {repr(ch)}\")\n",
        "\n",
        "    model.eval()\n",
        "    generated = list(seed_text)\n",
        "\n",
        "    input_ids = torch.tensor([char_to_id[ch] for ch in seed_text], device=DEVICE).unsqueeze(0)\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, hidden = model(input_ids, hidden)\n",
        "        current_id = input_ids[:, -1:]\n",
        "\n",
        "        for _ in range(num_tokens_to_generate):\n",
        "            logits, hidden = model(current_id, hidden)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            next_char = id_to_char[next_id.item()]\n",
        "            generated.append(next_char)\n",
        "            current_id = next_id\n",
        "\n",
        "    return \"\".join(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ed23c85",
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_text = \"MICHAEL: I have an idea for today's meeting.\\nJIM: \"\n",
        "\n",
        "samples = {}\n",
        "for temp in [0.3, 0.7, 1.0]:\n",
        "    samples[temp] = generate_script(\n",
        "        seed_text=seed_text,\n",
        "        temperature=temp,\n",
        "        num_tokens_to_generate=400,\n",
        "    )\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Temperature: {temp}\")\n",
        "    print(samples[temp])\n",
        "\n",
        "best_temp = 0.7\n",
        "best_sample = samples[best_temp]\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Best sample\")\n",
        "print(f\"Seed text: {seed_text}\")\n",
        "print(f\"Temperature: {best_temp}\")\n",
        "print(f\"Validation perplexity: {val_ppls[-1]:.4f}\")\n",
        "print(best_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6268ecdf",
      "metadata": {},
      "source": [
        "## Temperature Comparison Notes\n",
        "\n",
        "- **Coherence:** 0.3 is usually most coherent and on-format; 1.0 is most likely to drift.\n",
        "- **Creativity:** 1.0 tends to be more surprising/quirky; 0.7 balances novelty and structure.\n",
        "- **Repetition:** 0.3 can repeat phrases; 1.0 repeats less but can be noisier.\n",
        "- **Grammatical stability:** 0.3 is most stable, 0.7 acceptable, 1.0 may fragment.\n",
        "\n",
        "After running the notebook, update these notes if your observed samples differ."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "The_Office_Generator.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
